{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Double Checking Math\n\nThis notebook is based on the demo [Notebook](https://www.kaggle.com/code/windmaple/grpo-demo-gemma3-1b).\nIt was adapted in order to not only generate tags for reasoning and a solution, but also to subdivide the reasoning into a first reasoning with its solution and a critical evaluation of said first solution. The final solution is then outputted as always in the solution tag.\n\nBecause this notebook is adapted from said demo, it is based on [gemma3-1b](https://deepmind.google/models/gemma/) and the [gsm8k Math Dataset](https://huggingface.co/datasets/openai/gsm8k).\n\nThe model was (or is to be) trained with [GRPO](https://arxiv.org/pdf/2402.03300) and needs a 'v5e-8' TPU.","metadata":{"id":"abdhOBYHqYz6"}},{"cell_type":"code","source":"import os\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"KAGGLE_USERNAME\")\nos.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"KAGGLE_KEY\")\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n  kagglehub.login()\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\nos.environ[\"WANDB_CONSOLE\"] = \"off\"\nos.environ[\"WANDB_SILENT\"]=\"true\" \nos.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n!rm /kaggle/working/intermediate_ckpt/* -rf\n!rm /kaggle/working/ckpts/* -rf\n\nimport wandb\nwandb.init(mode='disabled')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T19:11:00.376694Z","iopub.execute_input":"2026-01-08T19:11:00.377926Z","iopub.status.idle":"2026-01-08T19:11:11.788975Z","shell.execute_reply.started":"2026-01-08T19:11:00.377890Z","shell.execute_reply":"2026-01-08T19:11:11.787939Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/q7hgtz4o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7dfd73cdc1d0>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Install necessary libraries","metadata":{"id":"afofSj37qYz6"}},{"cell_type":"code","source":"!pip install -q kagglehub\n\n!pip install -q ipywidgets\n\n!pip install -q tensorflow\n!pip install -q tensorflow_datasets\n!pip install -q tensorboardX\n!pip install -q transformers\n!pip install -q grain\n!pip install \"google-tunix[prod]==0.1.3\"\n\n# !pip install -q git+https://github.com/google/tunix\n# !pip install -q git+https://github.com/google/qwix\n\n!pip uninstall -q -y flax\n# !pip install -U flax\n!pip install flax==0.12.0\n\n!pip install -q datasets wandb==0.22.0","metadata":{"id":"Z03GnyApTn1j","outputId":"ccd485a1-84b1-4d9f-bf45-4bf4a4c8ee7b","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:21:08.383645Z","iopub.execute_input":"2026-01-08T08:21:08.383811Z","iopub.status.idle":"2026-01-08T08:21:43.817251Z","shell.execute_reply.started":"2026-01-08T08:21:08.383794Z","shell.execute_reply":"2026-01-08T08:21:43.816288Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"LnF9ZACiTn1k"}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\n\nimport csv\nimport shutil\n\nfrom flax import nnx\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm.auto import tqdm\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n# from tunix.models.gemma3 import model as gemma_lib\n# from tunix.models.gemma3 import params as params_lib\nfrom tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom datasets import load_dataset","metadata":{"id":"McTNo_r8Tn1k","outputId":"67f038b8-509b-46ed-b027-9b72ed7b628c","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:21:43.817839Z","iopub.execute_input":"2026-01-08T08:21:43.818014Z","iopub.status.idle":"2026-01-08T08:22:09.793547Z","shell.execute_reply.started":"2026-01-08T08:21:43.817997Z","shell.execute_reply":"2026-01-08T08:22:09.792650Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameters\n\nWe adapted the hyperparamters from the demo notebook to our increased response length.\nIf further computational ressource are available, you can increase **NUM_BATCHES** to its limit of '3738' and increase the **NUM_GENERATIONS** and **TRAIN_MICRO_BATCH_SIZE**","metadata":{"id":"Eu_NI9nHTn1k"}},{"cell_type":"code","source":"# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 64\nALPHA = 64.0\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO ======\n# === Generation during GRPO training ===\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 512\n# Important to keep a high-ish temperature for varied, diverse responses during\n# training.\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\n# The number of times the policy generates multiple responses for a given prompt\n# within a single training step. This corresponds to `G` in Algorithm 1 in the\n# paper. The \"group\" in GRPO comes from here.\nNUM_GENERATIONS = 3\n\n# === other GRPO configs ===\n# The number of iterations per batch (ùúá in GRPO algo 1).\nNUM_ITERATIONS = 1\n# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n# Important to keep a high enough value for this, otherwise, the KL divergence\n# can increase unchecked.\nBETA = 0.08\n# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n# stable updates.\nEPSILON = 0.2\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 3\n# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\nNUM_BATCHES = 3000 # 3738\n# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n# increased to a max. of 330 (if batch size is 4).\nNUM_TEST_BATCHES = 100 # 100\nEVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\nNUM_EPOCHS = 1  # can potentially train for more epochs\n\n# Number of training steps.\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n\n# === AdamW, warmup, cosine scheduler ===\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\n# == Cosine decay with warmup scheduler ==\n# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n# steps, and then gradually decrease the learning rate to 0 using cosine\n# scheduler.\nWARMUP_STEPS = 0.1 * MAX_STEPS\n# == Grad clipping ==\n# Grad clipping to prevent large gradients. Found this\n# important to keep KL divergence in check.\nMAX_GRAD_NORM = 0.1\n\n# Checkpoint saving\nINTERMEDIATE_CKPT_DIR = \"/kaggle/working/intermediate_ckpt/\"\nCKPT_DIR = \"/kaggle/working/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    # greedy search\n    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n    # some randomness\n    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    # liberal\n    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n}","metadata":{"id":"ZPPKme47Tn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:09.794235Z","iopub.execute_input":"2026-01-08T08:22:09.794649Z","iopub.status.idle":"2026-01-08T08:22:09.800193Z","shell.execute_reply.started":"2026-01-08T08:22:09.794632Z","shell.execute_reply":"2026-01-08T08:22:09.799521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utility functions","metadata":{"id":"ngjtE-63Tn1k"}},{"cell_type":"code","source":"def show_hbm_usage():\n  \"\"\"Displays memory usage per device.\"\"\"\n  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n\n  for d in jax.local_devices():\n    stats = d.memory_stats()\n    used = stats[\"bytes_in_use\"]\n    limit = stats[\"bytes_limit\"]\n    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")","metadata":{"id":"wjMFOr7aTn1k","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:22:09.800915Z","iopub.execute_input":"2026-01-08T08:22:09.801080Z","iopub.status.idle":"2026-01-08T08:22:09.822589Z","shell.execute_reply.started":"2026-01-08T08:22:09.801066Z","shell.execute_reply":"2026-01-08T08:22:09.821541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preprocessing\n\nThis part is the main difference between our approach and the classic one used in the demo.\nInstead of just encapsulating the reasoning in reasoning tags, we further subdivide reasoning into the following strucutre:\n\n{**reasoning_start**}\n\n  {**reasoning_first_start**}  \n  \n    To solve 7 √ó 6, I recall that multiplication is repeated addition. 7 √ó 6 means adding 7 six times: 7 + 7 + 7 + 7 + 7 + 7.\n    Calculating step by step: 7 + 7 = 14, 14 + 7 = 21, 21 + 7 = 28, 28 + 7 = 35, 35 + 7 = 42. So my first estimate is 42.\n  \n  {**reasoning_first_end**}\n\n  {**answer_first_start**}  \n  \n    42\n    \n  {**answer_first_end**}\n\n  {**reasoning_final_start**}\n  \n    I will double-check my calculation. 7 √ó 6 can also be seen as 6 √ó 7. Adding 6 seven times: 6 + 6 = 12, 12 + 6 = 18, 18 + 6 = 24, 24 + 6 = 30, 30 + 6 = 36, 36     + 6 = 42.This confirms that my initial answer of 42 is correct.\n    \n  {**reasoning_final_end**}\n    \n{**reasoning_end**}\n","metadata":{"id":"6BtpYMlaTn1k"}},{"cell_type":"code","source":"reasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\n\nreasoning_first_start = \"<reasoning_first>\"\nreasoning_first_end = \"</reasoning_first>\"\n\nreasoning_final_start = \"<reasoning_final>\"\nreasoning_final_end = \"</reasoning_final>\"\n\nanswer_first_start = \"<answer_first>\"\nanswer_first_end = \"</answer_first>\"\n\n\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n\nSYSTEM_PROMPT = f\"\"\"You are given a problem and you need to solve it in a strucuted reasoning manner.\n\nFirst you will write a reasoning block, starting with {reasoning_start} and ending with {reasoning_end}, in which you try to solve the problem based on your reasoning passed inside {reasoning_first_start} and {reasoning_first_end}, and then give a first solution inside {answer_first_start} and {answer_first_end}. You will then directly try to critically evaluate this solution inside {reasoning_final_start} and {reasoning_final_end} and give a second reasoning. This marks the end of your reasoning block.\n\nNow you will give your final solution based on this block inside {solution_start} and {solution_end}.\n\nExample:\n\n{reasoning_start}\n    {reasoning_first_start}\n        To solve 7 √ó 6, I recall that multiplication is repeated addition. 7 √ó 6 means adding 7 six times: 7 + 7 + 7 + 7 + 7 + 7.\n        Calculating step by step: 7 + 7 = 14, 14 + 7 = 21, 21 + 7 = 28, 28 + 7 = 35, 35 + 7 = 42. So my first estimate is 42.\n    {reasoning_first_end}\n\n    {answer_first_start}\n        42\n    {answer_first_end}\n\n    {reasoning_final_start}\n        I will double-check my calculation. 7 √ó 6 can also be seen as 6 √ó 7. Adding 6 seven times: 6 + 6 = 12, 12 + 6 = 18, 18 + 6 = 24, 24 + 6 = 30, 30 + 6 = 36, 36 + 6 = 42.\n        This confirms that my initial answer of 42 is correct.\n    {reasoning_final_end}\n{reasoning_end}\n\n{solution_start}\n  42\n{solution_end}\n\nMake sure:\n- Each reasoning section contains detailed thought process.\n- Each answer contains exactly one numerical value and no extra text.\n- If the first answer is correct, you are allowed to repeat it after proof.\n\nAlso: Keep your reasoning short and preicse! Try to focus on the important parts that need to be mentioned.\n\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"id":"h6RGv1kSTn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:09.823072Z","iopub.execute_input":"2026-01-08T08:22:09.823227Z","iopub.status.idle":"2026-01-08T08:22:09.831029Z","shell.execute_reply.started":"2026-01-08T08:22:09.823213Z","shell.execute_reply":"2026-01-08T08:22:09.830184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TEMPLATE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:09.831491Z","iopub.execute_input":"2026-01-08T08:22:09.831664Z","iopub.status.idle":"2026-01-08T08:22:09.847708Z","shell.execute_reply.started":"2026-01-08T08:22:09.831651Z","shell.execute_reply":"2026-01-08T08:22:09.846866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems.\nWhile this approach is also applicaple to other types of questions, they need to be somewhat objective in their answers and comparable, as the reward functions will reward based on the type of improvement between both solutions.\n\nAs there were only limited ressource available, we focues only on the math questions as they were the most optimal area to evaluate if our approach even works.","metadata":{"id":"WASP9N5JTn1k"}},{"cell_type":"code","source":"def extract_hash_answer(text: str) -> str | None:\n  if \"####\" not in text:\n    return None\n  return text.split(\"####\")[1].strip()\n\n\ndef _load_from_tfds(data_dir: str, split: str):\n  import tensorflow_datasets.text.gsm8k\n  return tfds.data_source(\n      \"gsm8k\",\n      split=split,\n      data_dir=data_dir,\n      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n      download=True,\n  )\n\n\ndef download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n  os.makedirs(target_dir, exist_ok=True)\n  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n  src = Path(src)\n  dst = Path(target_dir)\n\n  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n    shutil.copy2(csv_file, dst / csv_file.name)\n    print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n  return target_dir\n\n\ndef get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n  # Download data\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  if source == \"tfds\":\n    import tensorflow_datasets.text.gsm8k\n    data = tfds.data_source(\n        \"gsm8k\",\n        split=split,\n        data_dir=data_dir,\n        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n        download=True,\n    )\n\n  elif source == \"kaggle\":\n    kaggle_dir = download_kaggle_dataset(data_dir)\n    file_name = \"main_\" + split + \".csv\"\n    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n\n    data = []\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n      reader = csv.DictReader(csvfile)\n      for row in reader:\n        data.append({\n            \"question\": row[\"question\"],\n            \"answer\": row[\"answer\"],\n        })\n\n  elif source == \"huggingface\":    \n    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n    data = load_dataset(\"gsm8k\", \"main\", split=split)\n      \n  else:\n    raise ValueError(f\"Unknown source: {source}\")\n\n  def _as_text(v):\n    return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n  dataset = (\n      grain.MapDataset.source(data)\n      .shuffle(seed=42)\n      .map(\n          lambda x: {\n              # passed to model forward pass\n              \"prompts\": TEMPLATE.format(\n                  system_prompt=SYSTEM_PROMPT,\n                  question=_as_text(x[\"question\"]),\n              ),\n              # passed to reward functions\n              \"question\": _as_text(x[\"question\"]),\n              # passed to reward functions\n              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n          }\n      )\n  )\n  return dataset","metadata":{"id":"gTGjcSMNTn1k","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:22:09.848294Z","iopub.execute_input":"2026-01-08T08:22:09.848450Z","iopub.status.idle":"2026-01-08T08:22:09.857741Z","shell.execute_reply.started":"2026-01-08T08:22:09.848437Z","shell.execute_reply":"2026-01-08T08:22:09.856906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We split the dataset set into train and test sets as usual.","metadata":{"id":"uDwobMu_okwv"}},{"cell_type":"code","source":"# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\nsource = \"huggingface\"\n\nif source not in (\"tfds\", \"kaggle\", \"huggingface\"):\n  print(\"Invalid choice. Defaulting to 'tfds'.\")\n  source = \"\"\n\nprint(f\"Using data source: {source}\")\n\ndataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_BATCHES\n]\n\nif TRAIN_FRACTION == 1.0:\n  train_dataset = dataset.repeat(NUM_EPOCHS)\n  val_dataset = None\nelse:\n  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n\n  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n\ntest_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n    :NUM_TEST_BATCHES\n]\n\ndataset_lengths = (\n    len(train_dataset),\n    len(val_dataset) if val_dataset is not None else 0,\n    len(test_dataset),\n)\nprint(f\"dataset contains {dataset_lengths} of batches\")","metadata":{"id":"KXhOL6GyTn1k","outputId":"5e15f893-33eb-42e9-f4bd-20be01f2314a","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:22:09.858230Z","iopub.execute_input":"2026-01-08T08:22:09.858390Z","iopub.status.idle":"2026-01-08T08:22:13.311493Z","shell.execute_reply.started":"2026-01-08T08:22:09.858377Z","shell.execute_reply":"2026-01-08T08:22:13.310353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see how one batch of the training dataset looks like!\n","metadata":{"id":"k7n8L0VzTn1k"}},{"cell_type":"code","source":"for ele in train_dataset[:1]:\n  pprint(ele)","metadata":{"id":"5TF-wNQ2Tn1k","outputId":"367cd3ef-9b1c-469d-b50c-71887b040e87","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:13.312164Z","iopub.execute_input":"2026-01-08T08:22:13.312600Z","iopub.status.idle":"2026-01-08T08:22:13.318802Z","shell.execute_reply.started":"2026-01-08T08:22:13.312583Z","shell.execute_reply":"2026-01-08T08:22:13.317817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the policy model and the reference model\n\nThe policy model is the model which is actually trained and whose weights are\nupdated. The reference model is the model with which we compute KL divergence.\nThis is to ensure that the policy updates are not huge and that it does not\ndeviate too much from the reference model.\n\nTypically, the reference model is the base model, and the policy model is the\nsame base model, but with LoRA parameters. Only the LoRA parameters are updated.\n\nNote: We perform full precision (fp32) training. You can, however, leverage\nQwix for QAT.\n\nTo load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\nto have agreed to the Gemma license\n[here](https://www.kaggle.com/models/google/gemma/flax/).","metadata":{"id":"BZxBR7Y_Tn1k"}},{"cell_type":"code","source":"# Log in\n","metadata":{"id":"3GfLHHVYHHKO","outputId":"3290c9e8-2362-44a1-93bf-9a5caa7d201f","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:13.319178Z","iopub.execute_input":"2026-01-08T08:22:13.319336Z","iopub.status.idle":"2026-01-08T08:22:13.334333Z","shell.execute_reply.started":"2026-01-08T08:22:13.319322Z","shell.execute_reply":"2026-01-08T08:22:13.333452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n\nWe first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model.","metadata":{"id":"nAghcsT_Pmv_"}},{"cell_type":"code","source":"\nimport wandb\nwandb.init()\n!wandb login\n\n!rm /kaggle/working/intermediate_ckpt/* -rf\n\n!rm /kaggle/working/ckpts/* -rf\n\nmodel_family = \"gemma3\"\nif model_family == \"gemma3\":\n  MODEL_CP_PATH = params.GEMMA3_1B_IT\n  config = model.ModelConfig.gemma3_1b()\n  gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n  tokenizer = params.create_tokenizer()\n\n  checkpointer = ocp.StandardCheckpointer()\n  _, state = nnx.split(gemma)\n  checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n  checkpointer.wait_until_finished()\n  # Delete the intermediate model to save memory.\n  del params\n  del gemma\n  del state\n  gc.collect()","metadata":{"id":"cIFAxgVOTn1k","outputId":"0235a0e1-9f7d-428c-c16e-0bc9e49c2f2f","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:22:13.334812Z","iopub.execute_input":"2026-01-08T08:22:13.334964Z","iopub.status.idle":"2026-01-08T08:23:08.787254Z","shell.execute_reply.started":"2026-01-08T08:22:13.334951Z","shell.execute_reply":"2026-01-08T08:23:08.786262Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Loading and LoRA Application\n\nThese two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n\n* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training.","metadata":{"id":"hpgXONuORkkq"}},{"cell_type":"code","source":"from tunix.models.gemma3 import params\n\ndef get_gemma_ref_model(ckpt_path):\n  mesh = jax.make_mesh(*MESH)\n  model_config = model.ModelConfig.gemma3_1b()\n  abs_gemma: nnx.Module = nnx.eval_shape(\n      lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n  )\n\n  abs_state = nnx.state(abs_gemma)\n  abs_state = jax.tree.map(\n      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n      abs_state,\n      nnx.get_named_sharding(abs_state, mesh),\n  )\n  checkpointer = ocp.StandardCheckpointer()\n  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n\n  graph_def, _ = nnx.split(abs_gemma)\n  gemma = nnx.merge(graph_def, restored_params)\n  return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n  lora_provider = qwix.LoraProvider(\n      module_path=(\n          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n          \".*attn_vec_einsum\"\n      ),\n      rank=RANK,\n      alpha=ALPHA,\n  )\n\n  model_input = base_model.get_model_input()\n  lora_model = qwix.apply_lora_to_model(\n      base_model, lora_provider, **model_input\n  )\n\n  with mesh:\n    state = nnx.state(lora_model)\n    pspecs = nnx.get_partition_spec(state)\n    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(lora_model, sharded_state)\n\n  return lora_model","metadata":{"id":"m2KD-nmbTn1k","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:23:08.787814Z","iopub.execute_input":"2026-01-08T08:23:08.788035Z","iopub.status.idle":"2026-01-08T08:23:08.794312Z","shell.execute_reply.started":"2026-01-08T08:23:08.788018Z","shell.execute_reply":"2026-01-08T08:23:08.793551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we load reference and policy Gemma models using the Flax NNX library and display their structures.","metadata":{"id":"mgBALRieR6aY"}},{"cell_type":"code","source":"# Reference model\nif model_family == \"gemma3\":\n  ref_model, mesh, model_config = get_gemma_ref_model(\n      ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n  )","metadata":{"id":"kSdZ7aGhTn1k","outputId":"a536819f-dd5f-4e29-8ebe-09234960c114","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:23:08.794694Z","iopub.execute_input":"2026-01-08T08:23:08.794865Z","iopub.status.idle":"2026-01-08T08:23:14.281951Z","shell.execute_reply.started":"2026-01-08T08:23:08.794851Z","shell.execute_reply":"2026-01-08T08:23:14.280845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Policy model\nlora_policy = get_lora_model(ref_model, mesh=mesh)\n# nnx.display(lora_policy)","metadata":{"id":"4i3CfJ1gTn1k","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T08:23:14.282491Z","iopub.execute_input":"2026-01-08T08:23:14.282691Z","iopub.status.idle":"2026-01-08T08:23:22.300187Z","shell.execute_reply.started":"2026-01-08T08:23:14.282675Z","shell.execute_reply":"2026-01-08T08:23:22.299181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define reward functions\n\nWe define four reward functions:\n\n- reward if the format of the output exactly matches the instruction given in\n`TEMPLATE`;\n- reward if the format of the output approximately matches the instruction given\nin `TEMPLATE`;\n- reward if the answer is correct/partially correct;\n- reward if the final solution improves the inital solution;\n- Sometimes, the text between the solution tags might not be one\n  number. So, we extract the number, and reward the model if the answer is correct.\n\nThe reward functions are inspired from\n[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n\nFirst off, let's define a RegEx for checking whether the format matches.","metadata":{"id":"zLzR1tJfTn1k"}},{"cell_type":"code","source":"import re \nmatch_numbers = re.compile(\n    rf\"{answer_first_start}\\s*(.*?)\\s*{answer_first_end}\"  # Group 1: First Answer\n    r\".*?\"                                                # Non-greedy gap\n    rf\"{solution_start}\\s*(.*?)\\s*{solution_end}\", # Group 2: Final Answer\n    flags=re.DOTALL,\n)\n\ntest_prompt = f\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}1{answer_first_end}{solution_start}2{solution_end}\"\nm = match_numbers.search(\n    test_prompt\n)","metadata":{"id":"C7Beft8wTn1k","outputId":"a0ba4233-562d-485d-b9ba-2f22cf2785b4","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.300600Z","iopub.execute_input":"2026-01-08T08:23:22.300767Z","iopub.status.idle":"2026-01-08T08:23:22.304785Z","shell.execute_reply.started":"2026-01-08T08:23:22.300751Z","shell.execute_reply":"2026-01-08T08:23:22.304166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.305150Z","iopub.execute_input":"2026-01-08T08:23:22.305301Z","iopub.status.idle":"2026-01-08T08:23:22.322297Z","shell.execute_reply.started":"2026-01-08T08:23:22.305287Z","shell.execute_reply":"2026-01-08T08:23:22.321639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for group in m.groups():\n    print(group)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.322711Z","iopub.execute_input":"2026-01-08T08:23:22.322864Z","iopub.status.idle":"2026-01-08T08:23:22.329748Z","shell.execute_reply.started":"2026-01-08T08:23:22.322850Z","shell.execute_reply":"2026-01-08T08:23:22.328976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Give the model a reward of 4 points if the format matches exactly.","metadata":{"id":"Fe1rF15zTn1k"}},{"cell_type":"code","source":"NUMBER_RE = re.compile(r\"-?\\d+(\\.\\d+)?$\")\n\ndef match_numbers_exactly(prompts, completions, **kwargs):\n    scores = []\n\n    for response in completions:\n        m = match_numbers.search(response)\n        if m is None:\n            scores.append(0.0)\n            continue\n\n        answer_1 = m.group(1).strip()\n        answer_2 = m.group(2).strip()\n\n        # If not 2 numeric solutions exists, give 0.0 reward\n        if not NUMBER_RE.fullmatch(answer_1):\n            scores.append(0.0)\n            continue\n        if not NUMBER_RE.fullmatch(answer_2):\n            scores.append(0.0)\n            continue\n\n        # Passed all structural checks\n        scores.append(4.0)\n\n    return scores","metadata":{"id":"_fhQ6pY2Tn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.330090Z","iopub.execute_input":"2026-01-08T08:23:22.330257Z","iopub.status.idle":"2026-01-08T08:23:22.338930Z","shell.execute_reply.started":"2026-01-08T08:23:22.330244Z","shell.execute_reply":"2026-01-08T08:23:22.338272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [f\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}1{answer_first_end}{solution_start}f{solution_end}\",\n f\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}f{answer_first_end}{solution_start}f{solution_end}\",\nf\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}f{answer_first_end}{solution_start}f{solution_end}\",\nf\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}1{answer_first_end}{solution_start}f{solution_end}\",\nf\"{reasoning_start}Let methink!{reasoning_end}{answer_first_start}1{answer_first_end}{solution_start}2{solution_end}\"]\n\n\nfor prompt in prompts:\n    print(match_numbers_exactly(SYSTEM_PROMPT,[prompt]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.339398Z","iopub.execute_input":"2026-01-08T08:23:22.339579Z","iopub.status.idle":"2026-01-08T08:23:22.348056Z","shell.execute_reply.started":"2026-01-08T08:23:22.339564Z","shell.execute_reply":"2026-01-08T08:23:22.347349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also reward the model if the format of the output matches partially.","metadata":{"id":"sWdAdUHuTn1k"}},{"cell_type":"code","source":"# change to match tags as match numbers does not make sense\ndef match_numbers_approximately(prompts, completions, **kwargs):\n  scores = []\n\n  for completion in completions:\n    score = 0\n    response = completion\n    # Count how many keywords are seen - we penalize if too many!\n    # If we see 1, then plus some points!\n    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_first_start) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_first_end) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_final_start) == 1 else -0.5\n    score += 0.5 if response.count(reasoning_final_end) == 1 else -0.5\n    score += 0.5 if response.count(answer_first_start) == 1 else -0.5\n    score += 0.5 if response.count(answer_first_end) == 1 else -0.5\n    score += 0.5 if response.count(solution_start) == 1 else -0.5\n    score += 0.5 if response.count(solution_end) == 1 else -0.5\n    scores.append(score)\n  return scores","metadata":{"id":"uOhO4f3-Tn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.348454Z","iopub.execute_input":"2026-01-08T08:23:22.348636Z","iopub.status.idle":"2026-01-08T08:23:22.357310Z","shell.execute_reply.started":"2026-01-08T08:23:22.348621Z","shell.execute_reply":"2026-01-08T08:23:22.356599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [f\"{reasoning_start}{reasoning_first_start}Let methink!{reasoning_first_end}{answer_first_start}1{answer_first_end}{reasoning_final_start}I Think its like that!{reasoning_final_end}{reasoning_end}{solution_start}f{solution_end}\"]\n\nfor prompt in prompts:\n    print(match_numbers_approximately(SYSTEM_PROMPT,[prompt]))\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.357665Z","iopub.execute_input":"2026-01-08T08:23:22.357828Z","iopub.status.idle":"2026-01-08T08:23:22.369231Z","shell.execute_reply.started":"2026-01-08T08:23:22.357814Z","shell.execute_reply":"2026-01-08T08:23:22.368541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reward the model if the answer is correct and if it improved the initial answer. If the solution got worse, we punish the model. A reward is also given if the answer\ndoes not match exactly, i.e., based on how close the answer is to the correct\nvalue.","metadata":{"id":"A2fNZDgTTn1k"}},{"cell_type":"code","source":"def check_answer(prompts, completions, answer, **kwargs):\n    \"\"\"\n    completions: list of model outputs\n    answer: single true numerical answer (final correct answer)\n    \"\"\"\n    scores = []\n\n    for comp in completions:\n        match = match_numbers.search(comp)\n        if not match:\n            # Missing expected answer tags\n            scores.append(0.0)\n            continue\n\n        first_ans, final_ans = match.groups()\n        score = 0.0\n\n        try:\n            print(answer)\n            first_val = float(first_ans.strip())\n            final_val = float(final_ans.strip())\n            true_val = float(answer.strip())            \n            \n            # Score final answer\n            if final_val == true_val:\n                score += 3.0\n            elif 0.9 <= final_val / true_val <= 1.1:\n                score += 0.5\n            elif 0.8 <= final_val / true_val <= 1.2:\n                score += 0.25\n            else:\n                score -= 1.0\n\n            # Reward improvement or penalize regression\n            dist_first = abs(first_val - true_val)\n            dist_final = abs(final_val - true_val)\n            if dist_final < dist_first:\n                score += 0.5  # improvement bonus\n            elif dist_final > dist_first:\n                score -= 0.5  # regression penalty\n\n        except:\n            # Non-numeric answers\n            score = -0.5\n\n        scores.append(score)\n\n    return scores","metadata":{"id":"S8zcWsmhTn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.369537Z","iopub.execute_input":"2026-01-08T08:23:22.369695Z","iopub.status.idle":"2026-01-08T08:23:22.377733Z","shell.execute_reply.started":"2026-01-08T08:23:22.369680Z","shell.execute_reply":"2026-01-08T08:23:22.376977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt = f\"{reasoning_start}{reasoning_first_start}Let methink!{reasoning_first_end}{answer_first_start}1{answer_first_end}{reasoning_final_start}I Think its like that!{reasoning_final_end}{reasoning_end}{solution_start}2{solution_end}\"\ncheck_answer(SYSTEM_PROMPT,[test_prompt],answer=\"2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.377997Z","iopub.execute_input":"2026-01-08T08:23:22.378163Z","iopub.status.idle":"2026-01-08T08:23:22.391201Z","shell.execute_reply.started":"2026-01-08T08:23:22.378148Z","shell.execute_reply":"2026-01-08T08:23:22.390401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sometimes, the text between `<answer_first>` and `</answer_first>` and `<answer>` and `</answer>`might not be one\nnumber; it can be a sentence. So, we extract the number and compare the answer.","metadata":{"id":"nIpOVv78Tn1k"}},{"cell_type":"code","source":"match_numbers = re.compile(\n    rf\"{answer_first_start}\\s*(.+?)\\s*{answer_first_end}.*?\"\n    rf\"{solution_start}\\s*(.+?)\\s*{solution_end}\",\n    flags=re.DOTALL,\n)\n\ntext = f\"{answer_first_start} 1 {answer_first_end}{solution_start} 2 {solution_end}\"\nmatch = match_numbers.search(text)\n\nif match:\n    first_answer, final_answer = match.groups()\n    print(\"First answer:\", first_answer.strip())\n    print(\"Final answer:\", final_answer.strip())","metadata":{"id":"NXvRtbk8Tn1k","outputId":"1ab45f0e-d04a-455c-a046-7c7a1ec6ee22","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.391583Z","iopub.execute_input":"2026-01-08T08:23:22.391752Z","iopub.status.idle":"2026-01-08T08:23:22.399613Z","shell.execute_reply.started":"2026-01-08T08:23:22.391738Z","shell.execute_reply":"2026-01-08T08:23:22.398852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef check_numbers(prompts, completions, answer, **kwargs):\n    question = kwargs[\"question\"]\n    responses = completions\n    \n    # Extraction logic (ensure match_numbers is defined globally)\n    extracted_responses = [\n        (guess.group(1).strip(), guess.group(2).strip()) \n        if (guess := match_numbers.search(r)) is not None \n        else (None, None)\n        for r in responses\n    ]\n\n    scores = []\n    \n    # Helper to clean \"$\" and other symbols\n    def clean_val(text):\n        try:\n            return float(re.sub(r'[^\\d.]', '', text))\n        except:\n            return None\n\n    print(\"START ============================\")\n    \n    for (first_raw, final_raw), true_answer in zip(extracted_responses, answer):\n        # Default state\n        status = \"no_match\"\n        score = 0.0\n        \n        first_val = clean_val(first_raw)\n        final_val = clean_val(final_raw)\n        true_val = clean_val(str(true_answer))\n\n        if first_val is not None and final_val is not None and true_val is not None:\n            # 1. Base score: final answer correctness\n            is_correct = (final_val == true_val)\n            score = 1.5 if is_correct else 0.0\n\n            # 2. Distance comparison\n            dist_first = abs(first_val - true_val)\n            dist_final = abs(final_val - true_val)\n\n            if dist_final < dist_first:\n                status = \"improvement\"\n                score += 0.5\n            elif dist_final > dist_first:\n                status = \"got_worse\"\n                score -= 0.5\n            else:\n                # Distances are equal\n                status = \"keep_good\" if is_correct else \"keep_bad\"\n\n        scores.append(score)\n        \n        # Log the specific outcome for the first sample in the batch\n        if len(scores) == 1:\n            print(f\"Question: {question[0]}\")\n            print(f\"True Answer: {true_val}\")\n            print(f\"Full Response:\\n{responses[0]}\")\n            print(f\"Extracted: First={first_val}, Final={final_val}\")\n            print(f\"Outcome Category: {status}\")\n            print(f\"Score: {score}\")\n\n    print(\"END ==============================\")\n    return scores","metadata":{"id":"oxZQAFKOTn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.399977Z","iopub.execute_input":"2026-01-08T08:23:22.400148Z","iopub.status.idle":"2026-01-08T08:23:22.413294Z","shell.execute_reply.started":"2026-01-08T08:23:22.400134Z","shell.execute_reply":"2026-01-08T08:23:22.412540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_numbers(SYSTEM_PROMPT,[test_prompt],[\"2\"],question=[\"What is bigger, 1 or 2?\"])\ncheck_numbers(SYSTEM_PROMPT,[test_prompt],[\"1\"],question=[\"What is bigger, 1 or 2?\"])\ntest_prompt = f\"{reasoning_start}{reasoning_first_start}Let methink!{reasoning_first_end}{answer_first_start}2{answer_first_end}{reasoning_final_start}I Think its like that!{reasoning_final_end}{reasoning_end}{solution_start}2{solution_end}\"\ncheck_numbers(SYSTEM_PROMPT,[test_prompt],[\"2\"],question=[\"What is bigger, 1 or 2?\"])\ncheck_numbers(SYSTEM_PROMPT,[test_prompt],[\"1\"],question=[\"What is bigger, 1 or 2?\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.413585Z","iopub.execute_input":"2026-01-08T08:23:22.413748Z","iopub.status.idle":"2026-01-08T08:23:22.427400Z","shell.execute_reply.started":"2026-01-08T08:23:22.413733Z","shell.execute_reply":"2026-01-08T08:23:22.426743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\n\nBefore we train the model, let's evaluate the model on the test set so we can\nsee the improvement post training.\n\nWe evaluate it in two ways:\n\n**Quantitative**\n\n* **Keep Good**: percentage of samples for which the models first answer predicts the\ncorrect final numerical answer and the final answer keeps this value.\n* **Improvement**: percentage of samples for which the models first answer predicts the\nwrong final numerical answer and the final answer improves this value.\n* **Keep Bad**: percentage of samples for which the models first answer predicts the\nwrong final numerical answer and the final answer keeps this value.\n* **Got Worse**: percentage of samples for which the models first answer predicts the\ncorrect final numerical answer and the final answer dismisses this value.\n* **No Match**: percentage of samples for which the model outputs the\ncorrect format, i.e., reasoning between the reasoning special tokens, and the\nfinal answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n\n\n**Qualitative**\n\nWe'll also print outputs for a few given questions so that we can compare the generated output later.\n","metadata":{"id":"AaiYMJxFTn1k"}},{"cell_type":"markdown","source":"We define a helper function to generate an answer, given a prompt.","metadata":{"id":"HAaZ7NjBx99P"}},{"cell_type":"code","source":"def generate(\n    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n):\n  \"\"\"Given prompt, generates text.\"\"\"\n\n  if isinstance(question, str):\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=question,\n        ),\n    ]\n  else:\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=q,\n        )\n        for q in question\n    ]\n\n  out_data = sampler(\n      input_strings=input_batch,\n      max_generation_steps=768,\n      temperature=temperature,\n      top_k=top_k,\n      top_p=top_p,\n      echo=False,\n      seed=seed if seed is not None else None,\n      eos_tokens=[1,106],\n  )\n\n  output = out_data.text\n  if isinstance(question, str):\n    return output[0]\n  return output","metadata":{"id":"_k58bOicUHJy","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.427808Z","iopub.execute_input":"2026-01-08T08:23:22.427970Z","iopub.status.idle":"2026-01-08T08:23:22.434806Z","shell.execute_reply.started":"2026-01-08T08:23:22.427956Z","shell.execute_reply":"2026-01-08T08:23:22.434016Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Another helper function for evaluation, which also keeps track of the different categories of the double checkings result.","metadata":{"id":"zNoa5je7yJOt"}},{"cell_type":"code","source":"import re\n\ndef check_numbers_eval(prompts, completions, answer, **kwargs):\n\n    responses = completions\n    extracted_responses = [\n        (guess.group(1).strip(), guess.group(2).strip()) \n        if (guess := match_numbers.search(r)) is not None \n        else (None, None)\n        for r in responses\n    ]\n\n    scores = []\n    categories = []\n\n    def clean_val(text):\n        if text is None: return None\n        try:\n            # Remove symbols like $, commas, and units to allow float conversion\n            cleaned = re.sub(r'[^\\d.-]', '', text)\n            return float(cleaned)\n        except:\n            return None\n\n    for (first_raw, final_raw), true_answer in zip(extracted_responses, answer):\n        status = \"no_match\"\n        score = 0.0\n        \n        first_val = clean_val(first_raw)\n        final_val = clean_val(final_raw)\n        true_val = clean_val(str(true_answer))\n\n        if first_val is not None and final_val is not None and true_val is not None:\n            is_correct = (final_val == true_val)\n            # Base Reward for correct final answer\n            score = 1.5 if is_correct else 0.0\n            \n            dist_first = abs(first_val - true_val)\n            dist_final = abs(final_val - true_val)\n\n            if dist_final < dist_first:\n                status = \"improvement\"\n                score += 0.5\n            elif dist_final > dist_first:\n                status = \"got_worse\"\n                score -= 0.5\n            else:\n                status = \"keep_good\" if is_correct else \"keep_bad\"\n        \n        scores.append(score)\n        categories.append(status)\n        \n    return scores, categories","metadata":{"id":"yJo2nuKB-wlw","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.435077Z","iopub.execute_input":"2026-01-08T08:23:22.435246Z","iopub.status.idle":"2026-01-08T08:23:22.448344Z","shell.execute_reply.started":"2026-01-08T08:23:22.435231Z","shell.execute_reply":"2026-01-08T08:23:22.447599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\ndef evaluate_grpo_style(dataset, sampler, config):\n    total_score = 0\n    total_samples = 0\n    # Dictionary to track counts of: improvement, got_worse, keep_good, keep_bad, no_match\n    distribution = Counter()\n    \n    for batch in tqdm(dataset):\n        questions = batch[\"question\"]\n        answers = batch[\"answer\"]\n        \n        responses = generate(questions, sampler, **config)\n        \n        # Unpack both the numeric scores and the string categories\n        scores, categories = check_numbers_eval(\n            prompts=None, \n            completions=responses, \n            answer=answers, \n            question=questions\n        )\n        \n        for score, category in zip(scores, categories):\n            total_samples += 1\n            total_score += score\n            distribution[category] += 1\n            \n    avg_reward = total_score / total_samples\n    \n    # Calculate percentages for the distribution\n    dist_pct = {k: (v / total_samples) * 100 for k, v in distribution.items()}\n    \n    # Accuracy is the % of cases where the final answer was correct.\n    # In your logic, this is 'keep_good' plus any 'improvement' that resulted in a score >= 1.5\n    # A safer way is to just look at the scores we already have:\n    correct_final_count = sum(1 for s in scores if s >= 1.5) # You'd need to accumulate this in the loop\n    # Let's do it via the distribution for simplicity:\n    accuracy = dist_pct.get(\"keep_good\", 0) + dist_pct.get(\"improvement_if_correct\", 0) \n    # NOTE: If your 'improvement' category doesn't distinguish if the final was correct, \n    # stick to counting scores >= 1.5 inside the loop.\n    \n    # For now, let's return the full distribution dict so you can print it.\n    return avg_reward, total_samples, accuracy, dist_pct","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.448724Z","iopub.execute_input":"2026-01-08T08:23:22.448887Z","iopub.status.idle":"2026-01-08T08:23:22.460741Z","shell.execute_reply.started":"2026-01-08T08:23:22.448872Z","shell.execute_reply":"2026-01-08T08:23:22.460038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=1792,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"HZMO-KflTn1k","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.461019Z","iopub.execute_input":"2026-01-08T08:23:22.461183Z","iopub.status.idle":"2026-01-08T08:23:22.523624Z","shell.execute_reply.started":"2026-01-08T08:23:22.461168Z","shell.execute_reply":"2026-01-08T08:23:22.522872Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish.","metadata":{"id":"UOAQe06DyVlQ"}},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish.\n# We unpack the 4 values: Reward, Sample Count, Accuracy, and the Distribution Dict\navg_reward, total, accuracy, dist_pct = evaluate_grpo_style(\n    test_dataset,\n    sampler,\n    GENERATION_CONFIGS[\"greedy\"],\n)\n\n# Printing the summary\nprint(f\"\\n{'='*40}\")\nprint(f\"TUNIX CHALLENGE: GRPO EVALUATION\")\nprint(f\"{'='*40}\")\nprint(f\"Average Reward: {avg_reward:.4f}\")\nprint(f\"Total Samples:  {total}\")\nprint(f\"Final Accuracy: {accuracy:.2f}%\")\n\nprint(f\"\\nBehavior Distribution Breakdown:\")\n# We iterate through the categories to show how the model 'behaved'\nfor category in [\"keep_good\", \"improvement\", \"keep_bad\", \"got_worse\", \"no_match\"]:\n    percentage = dist_pct.get(category, 0.0)\n    # Formatting the label for readability\n    label = category.replace('_', ' ').title()\n    print(f\" - {label:15}: {percentage:.2f}%\")\n\nprint(f\"{'='*40}\")","metadata":{"id":"YQM-tzXWUmoE","outputId":"d125e6b1-f940-44b3-a05d-f837d2299f49","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:23:22.523892Z","iopub.execute_input":"2026-01-08T08:23:22.524038Z","iopub.status.idle":"2026-01-08T08:26:04.049109Z","shell.execute_reply.started":"2026-01-08T08:23:22.524025Z","shell.execute_reply":"2026-01-08T08:26:04.047962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train\n\nLet's set up all the configs first - checkpointing, metric logging and training.\nWe then train the model.","metadata":{"id":"-CmB2ZT9Tn1l"}},{"cell_type":"code","source":"# Ckpt saving\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logger\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n)","metadata":{"id":"mHzdsYsGTn1l","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:26:04.049621Z","iopub.execute_input":"2026-01-08T08:26:04.049802Z","iopub.status.idle":"2026-01-08T08:26:04.053557Z","shell.execute_reply.started":"2026-01-08T08:26:04.049785Z","shell.execute_reply":"2026-01-08T08:26:04.052799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer, learning rate scheduler, gradient clipping\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n  optimizer = optax.chain(\n      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n      optimizer,\n  )","metadata":{"id":"YWvBkWBsruom","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:26:04.054002Z","iopub.execute_input":"2026-01-08T08:26:04.054172Z","iopub.status.idle":"2026-01-08T08:26:04.063859Z","shell.execute_reply.started":"2026-01-08T08:26:04.054158Z","shell.execute_reply":"2026-01-08T08:26:04.063052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        # metrics logging\n        metrics_logging_options=metrics_logging_options,\n        # checkpoint saving\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=1536,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1,106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)","metadata":{"id":"_6VxFW1ZTn1l","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:26:04.064145Z","iopub.execute_input":"2026-01-08T08:26:04.064301Z","iopub.status.idle":"2026-01-08T08:26:04.073760Z","shell.execute_reply.started":"2026-01-08T08:26:04.064287Z","shell.execute_reply":"2026-01-08T08:26:04.073055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setting Up the GRPO Trainer\n\nNow we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n\nWe then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n\nTunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n\n**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n\n**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in.","metadata":{"id":"z4yJWiElSmOy"}},{"cell_type":"code","source":"\n# RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# GRPO Trainer\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_numbers_exactly,\n        match_numbers_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    grpo_config=grpo_config,\n)","metadata":{"id":"OIe1lO08Tn1l","outputId":"017a2a3f-d9fd-4ac8-87f5-760e2586dfc9","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:26:04.074045Z","iopub.execute_input":"2026-01-08T08:26:04.074208Z","iopub.status.idle":"2026-01-08T08:26:08.334013Z","shell.execute_reply.started":"2026-01-08T08:26:04.074194Z","shell.execute_reply":"2026-01-08T08:26:08.333089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!","metadata":{"id":"e8b71ed5"}},{"cell_type":"code","source":"with mesh:\n  grpo_trainer.train(train_dataset)","metadata":{"id":"S27XDebYTn1l","outputId":"2869f47f-c7d7-4bcc-b167-f05c77a619fa","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:26:08.334461Z","iopub.execute_input":"2026-01-08T08:26:08.334649Z","iopub.status.idle":"2026-01-08T12:44:58.893929Z","shell.execute_reply.started":"2026-01-08T08:26:08.334633Z","shell.execute_reply":"2026-01-08T12:44:58.892886Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\nLet's evaluate our finetuned model!","metadata":{"id":"FzIP8glkTn1l"}},{"cell_type":"code","source":"# Load checkpoint first.\nimport re\n\n# Find the latest checkpoint by listing directories in CKPT_DIR/actor\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n  for item in os.listdir(actor_ckpt_dir):\n    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n      step = int(item)\n      if step > latest_step:\n        latest_step = step\n\nif latest_step == -1:\n  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval')  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)","metadata":{"id":"V-73HfP1Tn1l","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:44:58.894233Z","iopub.execute_input":"2026-01-08T12:44:58.894413Z","iopub.status.idle":"2026-01-08T12:45:02.498085Z","shell.execute_reply.started":"2026-01-08T12:44:58.894397Z","shell.execute_reply":"2026-01-08T12:45:02.496955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=1792,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"1vY9kl-ITn1l","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:45:02.498802Z","iopub.execute_input":"2026-01-08T12:45:02.498975Z","iopub.status.idle":"2026-01-08T12:45:02.555853Z","shell.execute_reply.started":"2026-01-08T12:45:02.498960Z","shell.execute_reply":"2026-01-08T12:45:02.554961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish.\n# We unpack the 4 values: Reward, Sample Count, Accuracy, and the Distribution Dict\navg_reward, total, accuracy, dist_pct = evaluate_grpo_style(\n    test_dataset,\n    sampler,\n    GENERATION_CONFIGS[\"greedy\"],\n)\n\n# Printing the summary\nprint(f\"\\n{'='*40}\")\nprint(f\"TUNIX CHALLENGE: GRPO EVALUATION\")\nprint(f\"{'='*40}\")\nprint(f\"Average Reward: {avg_reward:.4f}\")\nprint(f\"Total Samples:  {total}\")\nprint(f\"Final Accuracy: {accuracy:.2f}%\")\n\nprint(f\"\\nBehavior Distribution Breakdown:\")\n# We iterate through the categories to show how the model 'behaved'\nfor category in [\"keep_good\", \"improvement\", \"keep_bad\", \"got_worse\", \"no_match\"]:\n    percentage = dist_pct.get(category, 0.0)\n    # Formatting the label for readability\n    label = category.replace('_', ' ').title()\n    print(f\" - {label:15}: {percentage:.2f}%\")\n\nprint(f\"{'='*40}\")","metadata":{"id":"nz0q_gGHqYz6","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:45:02.556138Z","iopub.execute_input":"2026-01-08T12:45:02.556293Z","iopub.status.idle":"2026-01-08T12:47:17.197610Z","shell.execute_reply.started":"2026-01-08T12:45:02.556280Z","shell.execute_reply":"2026-01-08T12:47:17.196835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we zip the model in order to be able to download it in Kaggle.","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/results/trained_model\", \"zip\", \"/kaggle/working/ckpts\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:47:17.197958Z","iopub.execute_input":"2026-01-08T12:47:17.198142Z","iopub.status.idle":"2026-01-08T12:47:24.298928Z","shell.execute_reply.started":"2026-01-08T12:47:17.198126Z","shell.execute_reply":"2026-01-08T12:47:24.297746Z"}},"outputs":[],"execution_count":null}]}