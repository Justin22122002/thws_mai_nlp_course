# -*- coding: utf-8 -*-
"""NLP Translation Tutorial

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFNsQiwWwflUYMlBV5QqfYrWbit6YuS1

# HuggingFace Translation Tutorial - Modified for the NLP Course
Made by: Paul Falkenstein, Justin Groh, Sandro Paval, Peter Preinesberger

What we changed from the baseline notebook:
- actually made it runnable (hf version was broken because of deprecations)
- add explanations within the notebook itself
- changed task to English to German translation

Make sure to select a runtime with GPU (e.g. T4) in colab, or be prepared to wait several years :)

Install the Transformers, Datasets, and Evaluate libraries from HF to run this notebook.

Note: datasets==3.6.0, as the kde4 dataset we use makes use of a dataset scripts, support for which were removed in the recent version of datasets
"""

!pip install datasets==3.6.0 evaluate transformers[sentencepiece]
!pip install accelerate

"""## The KDE4 Dataset
KDE is a suite of Desktop Applications (mostly for the *nix family of OSs), and all of the labels of the UI (Tooltips, Button Texts, Application Names, Help and Documentation) have to be available in a bouqet of languages.

This dataset consists of exactly these strings in several languages. For this notebook we'll be working with the German to English version.

As we will see, certain words mean different things specifically in the UI context, and the pretrained model we start with fails to produce good translations. By fine-tuning on this dataset, performance for UI translation can be improved.

We load the KDE4 dataset from HF:
"""

from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="de", lang2="en")

"""For the purposes of this tutorial, the dataset size is too large (would take too long):"""

raw_datasets

raw_datasets["train"][0]

"""So we remove tooltips that are very short and only randomly select 15000 of those longer strings:"""

def filter_fn(example):
  return len(example["translation"]["en"]) > 20
filtered_dataset = raw_datasets["train"].filter(filter_fn).shuffle(seed=42).select(range(15000))
filtered_dataset

"""## Dataset Processing
Nothing unusual here, we split the data into a train and validation set for later.
"""

split_datasets = filtered_dataset.train_test_split(train_size=0.9, seed=20)
split_datasets

split_datasets["validation"] = split_datasets.pop("test")

"""## Peculiarities of German and English in UI Settings
Some examples here of where the pre-trained opus-mt (an open source language translation model) falls short.

Load the Opus-MT English-to-German model from HF:
"""

from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-de"
translator = pipeline("translation", model=model_checkpoint)

"""Here's the example:
The English "Run Operation for Current Item"
gets translated by the pre-trained model to "Betrieb f端r aktuelles Objekt ausf端hren".

"Betrieb" in German does mean "Operation", but more in the sense of the "Economic Operation of a Company", and definetely not the intended "Operation run by a UI widget".
"""

translator(
    "Run Operation for Current Item"
)

"""## Setting up Tokenization and Preprocessing
Just using the tokenizer that comes with the pre-trained model here, nothing special.
"""

from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-de"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")

"""For translation tasks, there's two different tokenizers for the source and target language.

We feed the (label, target) strings into the tokenizer jointly, it takes care of this automatically then.
"""

max_length = 128

def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["de"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs

"""Finally, we apply the tokenization over our subsampled dataset:"""

tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)

"""## The Training Loop

In this section, we'll be using a bog-standard pytorch training loop and AdamW to fine-tune our language model using our tokenized dataset. As you will see HF does a lot of abstraction for us, so this is pretty easy!

Firstly, we wrap our pretrained model checkpoint into a pytorch-compatible object.
"""

from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

"""A data collator takes care of the necessary within-batch padding of the tokenized input sequences, as sequences within a batch need to be of the same length."""

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

"""The BLEU score is used to evaluate translation quality during the validation phase."""

!pip install sacrebleu

import evaluate

metric = evaluate.load("sacrebleu")

"""Just setting up some standard pytoch DataLoaders now ..."""

from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

"""Accelerator would help you if you would run this on a GPU cluster (which we're not, so this isn't strictly neccessary)."""

from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)

"""Double check that you are using a GPU here."""

accelerator.device

"""We use a linear LR decay, and 3 training epochs, as in the original version of this notebook."""

from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

"""Folder to save the model to locally."""

output_dir = "kde4-en-to-de-accelerate"

"""A function to turn network outputs and labels (in tokenized state), to objects that can be compared by the BLEU metric.

This will be used in the validation loop to track progress on the translation task.
"""

import numpy as np
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels

"""Your standard PyTorch training loop:"""

from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training, really nothing special here
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        # generate a translated sequence for the current batch
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save locally
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)

"""We will now check that our example from before training actually has improved translation quality:"""

from transformers import pipeline

translator = pipeline("translation", model=output_dir)

translator(
    "Run Operation for Previous Item"
)

""""Operation f端r aktuellen Eintrag ausf端hren" is the more correct translation of "Run Operation for Current Item".

The model has evidently been somewhat adapted to the UI domain.
"""